% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ls_factor.R
\name{ls_factor}
\alias{ls_factor}
\title{Least Squares Estimation of Linear Panel Data Models with Interactive Fixed Effects}
\usage{
ls_factor(
  Y,
  X,
  R,
  lambda_known = NA,
  f_known = NA,
  report = "report",
  precision_beta = 10^(-8),
  method = "m1",
  start,
  repMIN,
  repMAX,
  M1 = 1,
  M2 = 0,
  DoF_adj = FALSE
)
}
\arguments{
\item{Y}{\eqn{N \times T} matrix of outcomes, where we assume a balanced panel, i.e. all elements of \code{Y} are known}

\item{X}{\eqn{K \times N \times T} tensor of regressors, where we assume a balanced panel, i.e. all elements of \code{X} are known}

\item{R}{A positive integer, indicates the number of interactive fixed effects in the estimation; this \code{R} does not include the number of known factors and loadings}

\item{lambda_known}{(Optional) \eqn{N \times Rex1} matrix of known factor loadings, e.g., \code{lambda_known = matrix(rep(1, N), nrow = N, ncol = 1)} to control standard time dummies.
Default is set to \code{lambda_known = matrix(NA, nrow = N, ncol = 0)}, i.e., there is no known factor loadings}

\item{f_known}{(Optional) \eqn{T \times Rex2} matrix of known factor, e.g., \code{f_known = matrix(rep(1, T), nrow = T, ncol = 1)} to control standard individual specific fixed effects.
Default is set to \code{f_known = matrix(NA, nrow = T, ncol = 0)}, i.e., there is no known factor}

\item{report}{(Optional) Whether or not to report the progress. \code{"silent"} has the program running silently; \code{"report"} has the program reporting what it is doing}

\item{precision_beta}{(Optional) Defines stopping criteria for numerical optimization, namely optimization is stopped when difference in beta relative to previous opimtization step is smaller than \code{"precision_beta"} (uniformly over all \eqn{K} components of beta).
Note that the actual precision in beta will typically be lower than precision_beta, depending on the convergence rate of the procedure.}

\item{method}{(Optional) Optimization method option of choice. Options include \code{"m1"} and \code{"m2"}}

\item{start}{(Optional) \eqn{K \times 1} vector, first starting value for numerical optimization}

\item{repMIN}{(Optional) Minimal number, which is a positive integer, of runs of optimization with different starting point}

\item{repMAX}{(Optional) Maximal number, which is a positive integer, of runs of optimization (in case numerical optimization doesn't terminate properly, we do multiple runs even for \code{repMIN = 1})}

\item{M1}{(Optional) A positive integer, bandwidth for bias correction for dynamic bias (bcorr1), \code{M1} is the number of lags of correlation between regressors and errors that is corrected for in dynamic bias correction}

\item{M2}{(Optional) A non-negative integer, bandwidth for bias correction for time-serial correlation (\code{bcorr3}), \code{M2 = 0} only corrects for time-series heteroscedasticity, while \code{M2 > 0} corrects for time-correlation in errors up to lag \code{M2}}

\item{DoF_adj}{(Optional) Whether or not to adjust for degree of freedom, where default is set to \code{DoF_adj = FALSE}}
}
\value{
A list of results, where
\itemize{
\item \code{beta} is the parameter estimate
\item \code{exitflag = 1} if iteration algorithm properly converged at optimal beta, and \code{exitflag = -1} if iteration algorithm did not properly converge at optimal beta
\item \code{lambda} is the estimate for factor loading
\item \code{f} is the estimate for factors
\item \code{Vbeta1,2,3} are estimated variance-covariance matrices of beta, assuming
\enumerate{
\item homoscedasticity of errors in both dimensions
\item heteroscedasticity of errors in both dimensions
\item allowing for time-serial correlation up to lag M2 (i.e. if M2 == 0, then Vbeta2 == Vbeta3)
}
\item \code{bcorr1,2,3} are estimates for the three different bias components (needs to be subtracted from beta to correct for the bias), where
\enumerate{
\item is bias due to pre-determined regressors
\item is bias due to cross-sectional heteroscedasticity of errors
\item is bias due to time-serial heteroscedasticity and time-serial correlation of errors
}
\item \code{parameter} is the list of input parameters, including \code{Gamma_LS}, \code{alpha}, and \code{clustered_se}
}
}
\description{
This function estimates least squares estimator in a linear panel regression model with factors appearing as interactive fixed effects.
}
\details{
\strong{Disclaimer:} This function is translated and modified from the Matlab function \code{LS_factor.m} by Martin Weidner, and the documentation details are also mainly from the original function.
This code is offered with no guarantees. Not all features of this code were properly tested. Please let me know if you find any bugs or encounter any problems while using this code. All feedback is appreciated.

\strong{Different computational methods:}
\itemize{
\item Method 1 (recommended default method) iterates the following two steps until convergence:
\itemize{
\item Step 1: forgiven \code{beta} compute update for \code{lambda} and \code{f} as principal components of \code{Y - beta * X} (same as in method 1)
\item Step 2: forgiven \code{lambda} and \code{f} run a pooled OLS regression of \code{Y - lambda * t(f)} on \code{X} to update \code{beta}
}
\item Method 2 (described in Bai, 2009) iterates the following two steps until convergence:
\itemize{
\item Step 1: forgiven \code{beta} compute update for \code{lambda} and \code{f} as principal components of \code{Y - beta * X} (same as in method 1)
\item Step 2: forgiven \code{lambda} and \code{f} run a pooled OLS regression of \code{Y - lambda * t(f)} on \code{X} to update \code{beta}
}
The procedure is repeated multiple times with different starting values.
}

\strong{Comments:}
\itemize{
\item Another method would be to use step 1 as in Method 1 & 2, but to replace step 2 with a regression of \code{Y} on either \code{M_lambda * X} or \code{X * M_f}, i.e. to only project out either lambda or f in the step 2 regression.
Bai (2009) mentions this method and refers to Ahn, Lee, and Schmidt (2001), Kiefer (1980) and Sargan (1964) for this.
We have not tested this alternative method, but we suspect that Method 1 performs better in terms of speed of convergence.
\item This alternative method and the method proposed by Bai (2009), i.e. "method 2" here, have the property of reducing the LS objective function in each step.
This is not true for Method 1 and may be seen as a disadvantage of Method 1.
However, we found this to be a nice feature, because we use this property of Method 1 as a stopping rule:
if the LS objective function does not improve, then we know we are "far away" from a proper minimum, so we stop the iteration and begin the iteration with another randomly chosen starting value. Note that multiple runs with different starting values are required anyways for all methods (because the LS objective function may have multiple local minimal).
\item We recommend method 1, because each iteration step is fast and its rate of convergence in our tests was very good (faster than method 2).
However, we have not much explored the relative sensitivity of the different methods towards the choice of starting value. Note that by choosing the quickest method (method 1) one can try out more different starting values of the procedure in the same amount of time.
Nevertheless, it may well be that method 2 or the alternative method described above perform better in certain situations.
}
}
\note{
We assume that all provided input parameters have values and dimensions as described above.
The program could be improved by checking that this is indeed the case.
}
\examples{
dt <- sample_data(N = 100, T = 20, R = 1, kappa = c(0.5))
res <- ls_factor(Y = dt$Y, X = dt$X, R = dt$R, report = "silent",
                 precision_beta = 10^(-8), method = "m1",
                 start = c(0), repMIN = 3, repMAX = 10, M1 = 2, M2 = 2)
sum_res <- summary(res)
print(sum_res)

}
\references{
For a description of the model and the least squares estimator see e.g. Bai (2009, "Panel data models with interactive fixed effects"), or Moon and Weidner (2017, "Dynamic Linear Panel Regression Models with Interactive Fixed Effects"; 2015, "Linear Regression for Panel with Unknown Number of Factors as Interactive Fixed Effects")
}
